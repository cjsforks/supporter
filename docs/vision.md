Biblefed.org vision
===

**Maxwell's demon**  is a thought experiment showng "that the Second Law
of Thermodynamics has only a statistical certainty." [1]

The demon is a doorman. The demon is discerning. The demon decreases disorder.
However, the value of the doorman must exceed the wages of the doorman. [2]

A human doorman is capable of discerning only a finite number of people.
Robin Dunbar quantified this number to approximately 150. [3] A doorman exceeds the Dunbar number by using proxies. Proxies include badges,
diplomas, titles, uniforms, language, shiboleths ... and on the internet
superhighway; proxies empower gigascale computation.

However, humans remain limited in the ability to process information. And 
artifact proxies are inefficient. Internet communities leverage computing 
resources to crowd source discernment. Each member of a community digitally
records judgments and an algorithm compiles a curated "doorman's" list.

Google, Reddit, Hacker News, ... are doormen who discern through collective
intellegence a proxy decision. [4] Google, etc., are doormen for information.

Universities, traditionally, are also the doormen for information; and also
professional organizations, and increasingly ethical and legal standards. Keepers of religion, more anciently, are doormen to communities of ethical
behaviour.

This is the vision of **BibleFed**, an organization engine for communities.

Community Organization Engine
---

A cluster of computers can process more information than a single computer.
However, programming a cluster of computers involves a more complexity.
Similarly, a group of people can process more information than an individual.
However, group decisions are also subject to an additional level of complexity.

### Serial ###

A single computer processor core; like a single individual can process data in
series. Serial algorithms involves processing a series of steps. A series is
like a list. A list like the front page of Hacker News.

### Parallel ###

To leverage multiple processor cores; algorithms must be designed to operate
in parallel. Similarly; to leverage the collective intelligence of a group
of people requires a parallel algorithm. An common, if not essential, feature 
of parallel algorithms is a method of partitioning the data, i.e. breaking the
problem down into manageable pieces, or "batches."

### Trees ###










[1] Cargill Gilston Knott (1911). "Quote from undated letter from Maxwell to Tait". Life and Scientific Work of Peter Guthrie Tait. Cambridge University Press. p. 215.

[2] Bennett, C. H. (1982). "The thermodynamics of computation—a review". International Journal of Theoretical Physics 21 (12): 905–940.

[3] Dunbar, R. I. M. (1992). "Neocortex size as a constraint on group size in primates". Journal of Human Evolution 22 (6): 469–984.

[4] Abhinandan Das, et al., Google News Personalization: Scalable Online Collaborative Filtering. (May 2007)

